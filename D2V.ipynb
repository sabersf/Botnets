{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'Saber Shokat Fadaee'\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.doc2vec import TaggedDocument, LabeledSentence, Doc2Vec\n",
    "import gensim\n",
    "from sklearn import manifold, datasets\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import multiprocessing\n",
    "import csv\n",
    "import matplotlib as ml\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import re\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_checkerboard\n",
    "from sklearn.datasets import samples_generator as sg\n",
    "from sklearn.cluster.bicluster import SpectralBiclustering\n",
    "from sklearn.cluster.bicluster import SpectralCoclustering\n",
    "\n",
    "from sklearn.cluster.bicluster import SpectralCoclustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.externals.six import iteritems\n",
    "from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score\n",
    "from sklearn.utils.extmath import *\n",
    "from sklearn.metrics import consensus_score\n",
    "\n",
    "import operator\n",
    "\n",
    "storage = {}\n",
    "i = 1.0\n",
    "EID_set = set()\n",
    "botnet_set = set()\n",
    "event_set = set()\n",
    "\n",
    "\n",
    "file1 = open('EID.txt')\n",
    "for line in file1:\n",
    "        EID = line.strip()\n",
    "        EID_set.add(EID)\n",
    "file1.close()\n",
    "\n",
    "file1= open(\"botnets.txt\")\n",
    "for line in file1:\n",
    "    botnet = line.strip()\n",
    "    botnet_set.add(botnet)\n",
    "file1.close()\n",
    "\n",
    "EID_set = sorted(EID_set)\n",
    "botnet_set = sorted(botnet_set)\n",
    "event_set = sorted(event_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = np.loadtxt(\"count.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "botnet_family = []\n",
    "file1= open(\"bot_relations.txt\")\n",
    "for line in file1:\n",
    "    botnet_family.append(line.strip().split())\n",
    "file1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def in_list(item,L):\n",
    "    for i in L:\n",
    "        if item in i:\n",
    "            return L.index(i)\n",
    "    return 18\n",
    "def bot_to_vector(bot):\n",
    "    output = [0] * 19\n",
    "    output[in_list(bot, botnet_family)] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n",
      "0\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "15\n",
      "18\n",
      "17\n",
      "18\n",
      "3\n",
      "3\n",
      "18\n",
      "3\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "5\n",
      "1\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "17\n",
      "18\n",
      "18\n",
      "7\n",
      "18\n",
      "18\n",
      "18\n",
      "8\n",
      "8\n",
      "2\n",
      "18\n",
      "18\n",
      "17\n",
      "17\n",
      "18\n",
      "18\n",
      "18\n",
      "14\n",
      "13\n",
      "13\n",
      "18\n",
      "13\n",
      "18\n",
      "9\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "3\n",
      "17\n",
      "0\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "17\n",
      "16\n",
      "16\n",
      "16\n",
      "13\n",
      "17\n",
      "6\n",
      "6\n",
      "18\n",
      "18\n",
      "18\n",
      "1\n",
      "2\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "4\n",
      "4\n",
      "18\n",
      "18\n",
      "18\n",
      "17\n",
      "18\n",
      "18\n",
      "5\n",
      "18\n",
      "0\n",
      "0\n",
      "0\n",
      "18\n",
      "11\n",
      "18\n",
      "3\n",
      "17\n",
      "17\n",
      "17\n",
      "15\n",
      "9\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "12\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "17\n",
      "18\n",
      "18\n",
      "3\n",
      "18\n",
      "7\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "18\n",
      "17\n",
      "18\n",
      "12\n",
      "18\n",
      "18\n",
      "18\n",
      "3\n",
      "18\n",
      "18\n",
      "18\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for bot in botnet_set:\n",
    "    print in_list(bot, botnet_family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set colors to each category\n",
    "def sec_to_col(argument):\n",
    "    switcher = {\n",
    "\t\t'Aerospace/Defense': 'aqua',\n",
    "\t\t'Business Services': 'blueviolet',\n",
    "\t\t'Consumer Goods': 'brown',\n",
    "\t\t'Education': 'coral',\n",
    "\t\t'Energy/Resources': 'crimson',\n",
    "\t\t'Engineering': 'darkgreen',\n",
    "\t\t'Finance': 'gold',\n",
    "\t\t'Food Production': 'green',\n",
    "\t\t'Government/Politics': 'lime',\n",
    "\t\t'Healthcare/Wellness': 'magenta',\n",
    "\t\t'Insurance': 'mintcream',\n",
    "\t\t'Legal': 'olive',\n",
    "\t\t'Manufacturing': 'orchid',\n",
    "\t\t'Media/Entertainment': 'peru',\n",
    "\t\t'Nonprofit/NGO': 'purple',\n",
    "\t\t'Real Estate': 'red',\n",
    "\t\t'Retail': 'skyblue',\n",
    "\t\t'Technology': 'silver',\n",
    "\t\t'Telecommunications': 'tomato',\n",
    "\t\t'Tourism/Hospitality': 'peachpuff',\n",
    "\t\t'Transportation': 'rosybrown',\n",
    "\t\t'Unknown': 'dimgray',\n",
    "\t\t'Utilities': 'royalblue',\n",
    "    }\n",
    "    return switcher.get(argument, \"yellow\")\n",
    "\n",
    "\n",
    "\n",
    "#Set color to the different sizes\n",
    "\t\n",
    "def size_to_col(argument):\n",
    "    switcher = {\n",
    "\t\t'0-100': 'red',\n",
    "\t\t'100-1000': 'blue',\n",
    "\t\t'1000-10000': 'brown',\n",
    "\t\t'10000-50000': 'green',\n",
    "\t\t'50000+': 'gold',\n",
    "\t\t'Unknown': 'lime',\n",
    "    }\n",
    "    return switcher.get(argument, \"yellow\")\n",
    "\n",
    "# Assigns the topics to the documents in corpus\n",
    "\n",
    "col = []\n",
    "col_size = []\n",
    "\n",
    "sector = {}\n",
    "count_range = {}\n",
    "\n",
    "#Adding extra information\n",
    "with open('extra.csv', 'rb' ) as theFile:\n",
    "    reader = csv.DictReader( theFile )\n",
    "    for line in reader:\n",
    "\t\tind = int(line['']) \n",
    "\t\teid = line['entity_id_hash']\n",
    "\t\tsec = line['industry_sector']\n",
    "\t\tcnt = line['employee_count_range']\n",
    "\t\tsector[eid] = sec\n",
    "\t\tcount_range[eid] = cnt\n",
    "\n",
    "#Set numbers to each category\n",
    "def sec_to_num(argument):\n",
    "    switcher = {\n",
    "\t\t'Aerospace/Defense': 0,\n",
    "\t\t'Business Services': 1,\n",
    "\t\t'Consumer Goods': 2,\n",
    "\t\t'Education': 3,\n",
    "\t\t'Energy/Resources': 4,\n",
    "\t\t'Engineering': 5,\n",
    "\t\t'Finance': 6,\n",
    "\t\t'Food Production': 7,\n",
    "\t\t'Government/Politics': 8,\n",
    "\t\t'Healthcare/Wellness': 9,\n",
    "\t\t'Insurance': 10,\n",
    "\t\t'Legal': 11,\n",
    "\t\t'Manufacturing': 12,\n",
    "\t\t'Media/Entertainment': 13,\n",
    "\t\t'Nonprofit/NGO': 14,\n",
    "\t\t'Real Estate': 15,\n",
    "\t\t'Retail': 16,\n",
    "\t\t'Technology': 17,\n",
    "\t\t'Telecommunications': 18,\n",
    "\t\t'Tourism/Hospitality': 19,\n",
    "\t\t'Transportation': 20,\n",
    "\t\t'Unknown': 21,\n",
    "\t\t'Utilities': 22,\n",
    "    }\n",
    "    return switcher.get(argument, 23)\n",
    "#Set numbers to each size\n",
    "def size_to_num(argument):\n",
    "    switcher = {\n",
    "\t\t'0-100': 0,\n",
    "\t\t'100-1000': 1,\n",
    "\t\t'1000-10000': 2,\n",
    "\t\t'10000-50000': 3,\n",
    "\t\t'50000+': 4,\n",
    "\t\t'Unknown': 5,\n",
    "    }\n",
    "    return switcher.get(argument, 6)\n",
    "\n",
    "#Set category to each number\n",
    "def num_to_sec(argument):\n",
    "    switcher = {\n",
    "\t\t0:'Aerospace/Defense',\n",
    "\t\t1:'Business Services',\n",
    "\t\t2:'Consumer Goods',\n",
    "\t\t3:'Education',\n",
    "\t\t4:'Energy/Resources',\n",
    "\t\t5:'Engineering',\n",
    "\t\t6:'Finance',\n",
    "\t\t7:'Food Production',\n",
    "\t\t8:'Government/Politics',\n",
    "\t\t9:'Healthcare/Wellness',\n",
    "\t\t10:'Insurance',\n",
    "\t\t11:'Legal',\n",
    "\t\t12:'Manufacturing',\n",
    "\t\t13:'Media/Entertainment',\n",
    "\t\t14:'Nonprofit/NGO',\n",
    "\t\t15:'Real Estate',\n",
    "\t\t16:'Retail',\n",
    "\t\t17:'Technology',\n",
    "\t\t18:'Telecommunications',\n",
    "\t\t19:'Tourism/Hospitality',\n",
    "\t\t20:'Transportation',\n",
    "\t\t21:'Unknown',\n",
    "\t\t22:'Utilities',\n",
    "    }\n",
    "    return switcher.get(argument,23)\n",
    "#Set numbers to each size\n",
    "def num_to_size(argument):\n",
    "    switcher = {\n",
    "\t\t0:'0-100',\n",
    "\t\t1:'100-1000',\n",
    "\t\t2:'1000-10000',\n",
    "\t\t3:'10000-50000',\n",
    "\t\t4:'50000+',\n",
    "\t\t5:'Unknown',\n",
    "    }\n",
    "    return switcher.get(argument, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def included_entry(entry_name):\n",
    "    if sector[entry_name] == 'Education':\n",
    "        return False\n",
    "    if sector[entry_name] == 'Technology':\n",
    "        return False\n",
    "    if sector[entry_name] == 'Tourism/Hospitality':\n",
    "        return False\n",
    "    if sector[entry_name] == 'Telecommunications':\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3879"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(included_entry(entity) for entity in EID_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_new1 = np.zeros((207,sum(included_entry(entity) for entity in EID_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Build a new count matrix excluding the unwanted sectors\n",
    "index = 0\n",
    "EID_set_new = []\n",
    "for i in range(len(EID_set)):\n",
    "    if included_entry(EID_set[i]):\n",
    "        count_new1[:,index] = count[:,i]\n",
    "        EID_set_new.append(EID_set[i])\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_new = np.zeros((19,sum(included_entry(entity) for entity in EID_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(botnet_set)):\n",
    "    count_new[in_list(botnet_set[i], botnet_family) ,:] += count_new1[i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 3879)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group:  1 554588.0\n",
      "Group:  2 2.0\n",
      "Group:  3 0.0\n",
      "Group:  4 3628.0\n",
      "Group:  5 2086.0\n",
      "Group:  6 2539.0\n",
      "Group:  7 59694.0\n",
      "Group:  8 7167.0\n",
      "Group:  9 3.0\n",
      "Group:  10 150.0\n",
      "Group:  11 34789.0\n",
      "Group:  12 1297.0\n",
      "Group:  13 3.0\n",
      "Group:  14 3920.0\n",
      "Group:  15 844704.0\n",
      "Group:  16 6659.0\n",
      "Group:  17 12042.0\n",
      "Group:  18 3408069.0\n",
      "Group:  19 757302.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(19):\n",
    "    print \"Group: \", i+1, sum(count_new[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5698642.0\n"
     ]
    }
   ],
   "source": [
    "sum_count_new = 0\n",
    "for i in range(19):\n",
    "    sum_count_new += sum(count_new[i,:])\n",
    "print sum_count_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sectors_count(botnet_group):\n",
    "    sectors_count = [0]*23\n",
    "    res = dict()\n",
    "    for i in range(len(EID_set_new)):\n",
    "        if count_new[botnet_group,i] > 0:\n",
    "            sectors_count[sec_to_num(sector[EID_set_new[i]])] += count_new[botnet_group,i]\n",
    "    for i in range(23):\n",
    "        res[num_to_sec(i)] = sectors_count[i]\n",
    "    return res\n",
    "\n",
    "def sectors_count_botnet(bot):\n",
    "    sectors_count = [0]*23\n",
    "    for i in range(len(EID_set_new)):\n",
    "        if count_new1[bot,i] > 0:\n",
    "            sectors_count[sec_to_num(sector[EID_set_new[i]])] += count_new1[bot,i]\n",
    "    return sectors_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(18):\n",
    "    x = range(23)\n",
    "    y = sectors_count(i).values()\n",
    "    labels = sectors_count(i).keys()\n",
    "    plt.figure(figsize=(16,18))\n",
    "    plt.plot(x, y, 'r-')\n",
    "    plt.title((\"Group: %d. Contains botnets like: %s %s\")%(i+1,botnet_family[i][0],botnet_family[i][1]))\n",
    "    plt.xticks(x, labels, rotation='vertical')\n",
    "    plt.savefig(\"Group_%d.png\"%(i+1))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for bot in botnet_set:\n",
    "    output.append(bot_to_vector(bot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input1 = []\n",
    "for i in range(len(botnet_set)):\n",
    "    input1.append(sectors_count_botnet(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Initial input shape: (None, 23)\n",
      "--------------------------------------------------------------------------------\n",
      "Layer (name)                  Output Shape                  Param #             \n",
      "--------------------------------------------------------------------------------\n",
      "Dense (Unnamed)               (None, 32)                    768                 \n",
      "Activation (Unnamed)          (None, 32)                    0                   \n",
      "Dropout (Unnamed)             (None, 32)                    0                   \n",
      "Dense (Unnamed)               (None, 19)                    627                 \n",
      "Activation (Unnamed)          (None, 19)                    0                   \n",
      "--------------------------------------------------------------------------------\n",
      "Total params: 1395\n",
      "--------------------------------------------------------------------------------\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s - loss: 2.9559 - val_loss: 2.5048\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Epoch 1/2\n",
      "164/164 [==============================] - 0s - loss: 2.2765 - val_loss: 2.1127\n",
      "Epoch 2/2\n",
      "164/164 [==============================] - 0s - loss: 1.9943 - val_loss: 1.9653\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Epoch 1/3\n",
      "164/164 [==============================] - 0s - loss: 1.8870 - val_loss: 1.9377\n",
      "Epoch 2/3\n",
      "164/164 [==============================] - 0s - loss: 1.7830 - val_loss: 1.9222\n",
      "Epoch 3/3\n",
      "164/164 [==============================] - 0s - loss: 1.7953 - val_loss: 1.9159\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Epoch 1/4\n",
      "164/164 [==============================] - 0s - loss: 1.7669 - val_loss: 1.9166\n",
      "Epoch 2/4\n",
      "164/164 [==============================] - 0s - loss: 1.7472 - val_loss: 1.9030\n",
      "Epoch 3/4\n",
      "164/164 [==============================] - 0s - loss: 1.7429 - val_loss: 1.8964\n",
      "Epoch 4/4\n",
      "164/164 [==============================] - 0s - loss: 1.7389 - val_loss: 1.8945\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Epoch 1/5\n",
      "164/164 [==============================] - 0s - loss: 1.7066 - val_loss: 1.8980\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 0s - loss: 1.7035 - val_loss: 1.8989\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 0s - loss: 1.7051 - val_loss: 1.9080\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 0s - loss: 1.7045 - val_loss: 1.9100\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 0s - loss: 1.7133 - val_loss: 1.9178\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Epoch 1/6\n",
      "164/164 [==============================] - 0s - loss: 1.6570 - val_loss: 1.9182\n",
      "Epoch 2/6\n",
      "164/164 [==============================] - 0s - loss: 1.6926 - val_loss: 1.9373\n",
      "Epoch 3/6\n",
      "164/164 [==============================] - 0s - loss: 1.6859 - val_loss: 1.9223\n",
      "Epoch 4/6\n",
      "164/164 [==============================] - 0s - loss: 1.6889 - val_loss: 1.9456\n",
      "Epoch 5/6\n",
      "164/164 [==============================] - 0s - loss: 1.6727 - val_loss: 1.9496\n",
      "Epoch 6/6\n",
      "164/164 [==============================] - 0s - loss: 1.6472 - val_loss: 1.9587\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Epoch 1/7\n",
      "164/164 [==============================] - 0s - loss: 1.6358 - val_loss: 1.9584\n",
      "Epoch 2/7\n",
      "164/164 [==============================] - 0s - loss: 1.6547 - val_loss: 1.9675\n",
      "Epoch 3/7\n",
      "164/164 [==============================] - 0s - loss: 1.6674 - val_loss: 1.9725\n",
      "Epoch 4/7\n",
      "164/164 [==============================] - 0s - loss: 1.6379 - val_loss: 1.9740\n",
      "Epoch 5/7\n",
      "164/164 [==============================] - 0s - loss: 1.6484 - val_loss: 1.9873\n",
      "Epoch 6/7\n",
      "164/164 [==============================] - 0s - loss: 1.6370 - val_loss: 1.9902\n",
      "Epoch 7/7\n",
      "164/164 [==============================] - 0s - loss: 1.6317 - val_loss: 1.9938\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Epoch 1/8\n",
      "164/164 [==============================] - 0s - loss: 1.6278 - val_loss: 2.0181\n",
      "Epoch 2/8\n",
      "164/164 [==============================] - 0s - loss: 1.6432 - val_loss: 2.0316\n",
      "Epoch 3/8\n",
      "164/164 [==============================] - 0s - loss: 1.6125 - val_loss: 2.0341\n",
      "Epoch 4/8\n",
      "164/164 [==============================] - 0s - loss: 1.6330 - val_loss: 2.0285\n",
      "Epoch 5/8\n",
      "164/164 [==============================] - 0s - loss: 1.6057 - val_loss: 2.0329\n",
      "Epoch 6/8\n",
      "164/164 [==============================] - 0s - loss: 1.6034 - val_loss: 2.0321\n",
      "Epoch 7/8\n",
      "164/164 [==============================] - 0s - loss: 1.6302 - val_loss: 2.0304\n",
      "Epoch 8/8\n",
      "164/164 [==============================] - 0s - loss: 1.6295 - val_loss: 2.0381\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Epoch 1/9\n",
      "164/164 [==============================] - 0s - loss: 1.6055 - val_loss: 2.0319\n",
      "Epoch 2/9\n",
      "164/164 [==============================] - 0s - loss: 1.5942 - val_loss: 2.0325\n",
      "Epoch 3/9\n",
      "164/164 [==============================] - 0s - loss: 1.5945 - val_loss: 2.0361\n",
      "Epoch 4/9\n",
      "164/164 [==============================] - 0s - loss: 1.6022 - val_loss: 2.0409\n",
      "Epoch 5/9\n",
      "164/164 [==============================] - 0s - loss: 1.5901 - val_loss: 2.0478\n",
      "Epoch 6/9\n",
      "164/164 [==============================] - 0s - loss: 1.5861 - val_loss: 2.0504\n",
      "Epoch 7/9\n",
      "164/164 [==============================] - 0s - loss: 1.6032 - val_loss: 2.0516\n",
      "Epoch 8/9\n",
      "164/164 [==============================] - 0s - loss: 1.5582 - val_loss: 2.0582\n",
      "Epoch 9/9\n",
      "164/164 [==============================] - 0s - loss: 1.5595 - val_loss: 2.0580\n",
      "Train on 164 samples, validate on 42 samples\n",
      "Epoch 1/10\n",
      " 44/164 [=======>......................] - ETA: 0s - loss: 1.3083"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "inp = np.array(input1)\n",
    "out = np.array(output)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inp, out, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(23,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(19))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 4\n",
    "nb_classes = 3\n",
    "nb_epoch = 40\n",
    "target = open(\"NN_out\", 'w')\n",
    "for nb_epoch in range(50):\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(), class_mode=\"categorical\")\n",
    "\n",
    "    history = model.fit(X_train, Y_train,  batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                        verbose=1, validation_data=(X_test, Y_test))\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    #print('Test score:', score)\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    yy = np.argmax(p, axis=1)\n",
    "    yyy = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    a = np.equal(yy, yyy)\n",
    "    test_acc = ( 100.0 * (0.0 + sum(a)) / (len(a) + 0.0 ))\n",
    "\n",
    "    p = model.predict(X_train)\n",
    "    yy = np.argmax(p, axis=1)\n",
    "    yyy = np.argmax(Y_train, axis=1)\n",
    "\n",
    "    a = np.equal(yy, yyy)\n",
    "    train_acc = ( 100.0 * (0.0 + sum(a)) / (len(a) + 0.0 ))\n",
    "    target.write(\"NB_EPOCH : \" + str(nb_epoch) + \" Score: \" + str(score) + \" test accuracy: \" + str(test_acc) + \" Train accuracy: \"  + str(train_acc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
